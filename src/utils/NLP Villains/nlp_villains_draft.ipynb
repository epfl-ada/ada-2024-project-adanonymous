{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96ba9612-3d8f-4d55-b8b6-5dcb5ab5255a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd \n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "from tqdm import tqdm\n",
    "import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7a599",
   "metadata": {},
   "source": [
    "# Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee850026",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_pickle('../../../data/movies.pkl')\n",
    "plot_summaries = pd.read_pickle('../../../data/plot_summaries.pkl')\n",
    "tvtropes = pd.read_pickle('../../../data/tvtropes.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "162a59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = pd.read_csv('../../../data/character.metadata.tsv', sep='\\t')\n",
    "characters_columns = ['Wikipedia_Movie_ID', 'Freebase_Movie_ID', 'Movie_Release_Date', 'Character_Name', 'Actor_DOB', 'Actor_Gender', 'Actor_Height', 'Actor_Ethnicity', 'Actor_Name', 'Actor_Age_At_Movie_Release', 'Freebase_Character_Actor_Map_ID', 'Freebase_character_ID', 'Freebase_Actor_ID']\n",
    "characters.columns = characters_columns\n",
    "\n",
    "# Convert movie release date to movie release year\n",
    "movies['Movie_release_date'] = pd.to_datetime(movies['Movie_release_date'], format = 'mixed', errors='coerce', utc=True)\n",
    "movies['Movie_Release_Year'] = movies['Movie_release_date'].dt.year\n",
    "movies['Movie_Release_Year'] = movies['Movie_Release_Year'].fillna(0.0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9f7965",
   "metadata": {},
   "source": [
    "# Reducing the dataframe to keep only rows of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f588fa",
   "metadata": {},
   "source": [
    "We want to know wether the characters of the movies are positively or negatively seen. We can therefore remove all the rows for which we don't have any character, and all the rows that are just duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a30dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of characters_name_df after removing empty values:  192793\n",
      "Length of characters_name_df after removing duplicates:  126628\n"
     ]
    }
   ],
   "source": [
    "characters_name_df = characters[['Character_Name', 'Wikipedia_Movie_ID']]\n",
    "characters_name_df = characters_name_df.dropna()\n",
    "print(\"Length of characters_name_df after removing empty values: \", len(characters_name_df))\n",
    "# drop duplicates in the character names column\n",
    "characters_name_df = characters_name_df.drop_duplicates(subset='Character_Name')\n",
    "print(\"Length of characters_name_df after removing duplicates: \", len(characters_name_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0974e",
   "metadata": {},
   "source": [
    "We can also remove all the rows of the characters that do not appear in the plot summary. To do this, we first merge the plot summaries dataframe and the character names dataframe based on their Wikipedia_Movie_ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9886275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_summaries = plot_summaries.rename(columns={'Wikipedia_movie_ID': 'Wikipedia_Movie_ID'})\n",
    "names_and_plot = pd.merge(characters_name_df, plot_summaries, on='Wikipedia_Movie_ID', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1262b",
   "metadata": {},
   "source": [
    "We now check for each character name wether it appears in the plot summary or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bccad54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_character_in_plot(row):\n",
    "\n",
    "    if pd.isna(row['Plot']) or pd.isna(row['Character_Name']):\n",
    "        return False # If the plot or character name is missing, return False\n",
    "    \n",
    "    character_words = row['Character_Name'].split() # We want to check if at least a part of the \n",
    "                                                    # character name is in the plot, for example \n",
    "                                                    # \"Watson\" in \"Dr. Watson\". Note : no risk of\n",
    "                                                    # splitting the '.' alone, the split function\n",
    "                                                    # splits on spaces by default.\n",
    "\n",
    "    for word in character_words:\n",
    "        if word.lower() in row['Plot'].lower(): # We make sure to detect the character name, \n",
    "                                                # regardless of the case\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "names_and_plot['Character_in_plot'] = names_and_plot.apply(check_character_in_plot, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c584ad5a",
   "metadata": {},
   "source": [
    "Finally, we remove all the rows for which the character name does not appear in the plot summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86b96312",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_and_plot = names_and_plot[names_and_plot['Character_in_plot'] == True]\n",
    "names_and_plot = names_and_plot.drop(columns=['Character_in_plot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce43d3c",
   "metadata": {},
   "source": [
    "According to the movies genre analysis, we observed that certain specific genres have clear peaks during the period of the World War II. We will focus on these genres for our character analysis. \n",
    "\n",
    "TODO : Expliquer que l'idée c'est qu'on a vu qu'il y avait un pic pour certains genres de films. Maintenant qu'on a vu qu'il y a un effet sur le nombre de films dans certains genre, on veut voir si parmi ces films il y a un effet sur les personnages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49a2d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = movies.rename(columns={'Wikipedia_movie_ID': 'Wikipedia_Movie_ID'})\n",
    "names_plot_genres = pd.merge(names_and_plot, movies[['Wikipedia_Movie_ID', 'Movie_genres', 'Movie_Release_Year', 'Movie_name']], on='Wikipedia_Movie_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f50041c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_genre(df, genre):\n",
    "    return df[df['Movie_genres'].apply(lambda x: x is not None and genre in x)]\n",
    "\n",
    "def select_genres(df, genres):\n",
    "    return df[df['Movie_genres'].apply(lambda x: x is not None and any(genre in x for genre in genres))]\n",
    "\n",
    "def select_period(df, start, end):\n",
    "    return df[(df['Movie_Release_Year'] >= start) & (df['Movie_Release_Year'] <= end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3b2bcc",
   "metadata": {},
   "source": [
    "We are interested in the following genres, between 1930 and 1955 : Propaganda film, Combat Films, Suspense, Documentary, Psychological thriller, History, War film, Film noir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d134988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final length of the dataset:  1116\n"
     ]
    }
   ],
   "source": [
    "selected_period = names_plot_genres.copy()\n",
    "selected_period = select_period(selected_period, 1930, 1955)\n",
    "\n",
    "selected_genres = select_genres(selected_period, ['Propaganda film', 'Combat Films', 'Suspense', \n",
    "                                                  'Documentary', 'Psychological thriller', 'History', \n",
    "                                                  'War film', 'Film noir'])\n",
    "\n",
    "print(\"Final length of the dataset: \", len(selected_genres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5704c8f",
   "metadata": {},
   "source": [
    "For each characters of this preprocessed dataset, we want to know whether they are positively or negatively seen. The plot summary is not sufficient to do a meaningful sentiment analysis on the characters, so we will use an LLM that take into account extra information about the characters. We use the api of the LLM gpt, with the following prompt : \n",
    "\n",
    "\"-1 means negatively seen character. 0 means neutral. 1 means positively seen character. Please rate {character_name} from the movie {movie_name}. You can use your knowledge about the movie. One word answer\"\n",
    "\n",
    "Note : we removed from this notebook the api key we are using because it is a confidential key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75f1a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = 'ENTER-A-KEY-HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abe10e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_perception(character_name, movie_name):\n",
    "    prompt = f\"-1 means negatively seen character. 0 means neutral. 1 means positively seen character. Please rate {character_name} from the movie {movie_name}. You can use your knowledge about the movie. One word answer\"\n",
    "\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4o-mini\",  \n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=50,  \n",
    "            temperature=0.05  # Controls the randomness of the output, here we prefere a deterministic answer over an expressive one\n",
    "        )\n",
    "\n",
    "        perception_score = response.choices[0].message['content'].strip()\n",
    "\n",
    "        try:\n",
    "            score = int(perception_score)\n",
    "            if -10 <= score <= 10:\n",
    "                return score\n",
    "            else:\n",
    "                return \"Error: Invalid score range\"\n",
    "        except ValueError:\n",
    "            return \"Error: Unable to interpret response\"\n",
    "    \n",
    "    except openai.error.RateLimitError as e:\n",
    "        return \"Error: Rate limit exceeded. Please try again later.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    \n",
    "\n",
    "def add_perception_score(df):\n",
    "    df['Perception_score'] = df.apply(lambda x: get_character_perception(x['Character_Name'], x['Movie_name']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "061969a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_with_scores = add_perception_score(selected_genres)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8d858e",
   "metadata": {},
   "source": [
    "We keep only the characters that are negatively perceived. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "680d83e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_scores = pd.read_csv('../../../data/dataset_after_api_call.csv')\n",
    "villain_analysis = df_with_scores[df_with_scores['Perception_score'] < 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b3efd",
   "metadata": {},
   "source": [
    "Among the negatively seen characters, we want to see the words and characteristics associated with these characters on the plot summaries. To do this, we use a traditional nlp procedure as seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28adf607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "7479e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_associated_words(character_name, plot):\n",
    "\n",
    "    associated_words = []\n",
    "    characters_name_parts = character_name.lower().split()\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # load the spacy model in english\n",
    "    nlp_coref = spacy.load(\"en_coreference_web_trf\") # load the spacy model with coreference resolution\n",
    "    nlp_coref.replace_listeners(\"transformer\", \"coref\", [\"model.tok2vec\"])\n",
    "    nlp_coref.replace_listeners(\"transformer\", \"span_resolver\", [\"model.tok2vec\"])\n",
    "    nlp.add_pipe(\"coref\", source=nlp_coref)\n",
    "    nlp.add_pipe(\"span_resolver\", source=nlp_coref)\n",
    "\n",
    "    doc = nlp(plot) # Tokenization\n",
    "    coref_dict = doc.spans\n",
    "\n",
    "    nb_clusters = int(len(coref_dict)/2)\n",
    "    corref_clusters = []\n",
    "\n",
    "    for i in range(nb_clusters):\n",
    "        corref_list = coref_dict[f'coref_head_clusters_{i+1}']\n",
    "        corref_list = [token.text for token in corref_list]\n",
    "        corref_clusters.append(corref_list)\n",
    "\n",
    "    for corref_words in corref_clusters:\n",
    "        corref_words = [word.lower() for word in corref_words]\n",
    "        if any(name_part in corref_words for name_part in characters_name_parts):\n",
    "            corref_list = corref_words\n",
    "            break\n",
    "\n",
    "    corref_list = [word for word in corref_list if word not in nlp.Defaults.stop_words]\n",
    "    characters_name_parts.extend(corref_list)\n",
    "    characters_name_parts = list(set(characters_name_parts))\n",
    "\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        sentence_text = sentence.lower()\n",
    "\n",
    "        if any(name_part in sentence_text for name_part in characters_name_parts): # we want to focus on the sentences where the character appears\n",
    "        \n",
    "            for token in nlp(sentence_text):\n",
    "\n",
    "                token_word = token.text\n",
    "                token_head = token.head.text\n",
    "                token_children = [child.text for child in token.children]\n",
    "\n",
    "                # if token_word is refering the character, we want to keep the head and the children\n",
    "                if token_word.lower() in characters_name_parts:\n",
    "                    associated_words.append(token_head)\n",
    "                    associated_words.extend(token_children)\n",
    "\n",
    "                # if token_word is an adjective, we want to keep it if the head is refering the character\n",
    "                if token.pos_ == 'ADJ' and token_head.lower() in characters_name_parts:\n",
    "                    associated_words.append(token_word)\n",
    "\n",
    "                # if one of the children is reffering the character, we want to keep the other children\n",
    "                for child in token_children:\n",
    "                    if child.lower() in characters_name_parts:\n",
    "                        associated_words.extend(token_children)\n",
    "\n",
    "\n",
    "    associated_words = [word.lower() for word in associated_words]\n",
    "    associated_words.extend(characters_name_parts)\n",
    "\n",
    "    # We want to remove from the list : character name, stopwords, punctuation, and duplicates\n",
    "    associated_words = list(set(associated_words))\n",
    "    associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "    associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "    associated_words = [word for word in associated_words if word.isalpha()]\n",
    "\n",
    "    return associated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738f5380",
   "metadata": {},
   "source": [
    "# WAAAAAAOUUUUTCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8dbc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "\n",
    "def get_associated_words(character_name, plot):\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\") # load the spacy model in english\n",
    "    nlp_coref = spacy.load(\"en_coreference_web_trf\") # load the spacy model with coreference resolution\n",
    "    nlp_coref.replace_listeners(\"transformer\", \"coref\", [\"model.tok2vec\"])\n",
    "    nlp_coref.replace_listeners(\"transformer\", \"span_resolver\", [\"model.tok2vec\"])\n",
    "    nlp.add_pipe(\"coref\", source=nlp_coref)\n",
    "    nlp.add_pipe(\"span_resolver\", source=nlp_coref)\n",
    "\n",
    "    plot = \" \".join(plot.split()) # make sure there is only one space between words\n",
    "    plot = plot.lower() # lowercase\n",
    "    doc = nlp(plot) # Tokenization\n",
    "\n",
    "    coref_dict = doc.spans\n",
    "    \n",
    "    associated_words = []\n",
    "    characters_name_parts = character_name.lower().split()\n",
    "\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        sentence_doc = nlp(sentence)\n",
    "        sentence_text = sentence.lower()\n",
    "\n",
    "        \n",
    "\n",
    "        keep_sentence = (\n",
    "            any(name_part in sentence_text for name_part in characters_name_parts) or\n",
    "            any(token.pos_ == \"PRON\" and token._.in_coref for token in sentence_doc)\n",
    "        )\n",
    "\n",
    "        if character_name.lower() in sentence: # we want to focus on the sentences where the character appears\n",
    "            for token in nlp(sentence):\n",
    "                token_word = token.text\n",
    "                token_head = token.head.text\n",
    "                token_children = [child.text for child in token.children]\n",
    "\n",
    "                # if token_word is the character name, we want to keep the head and the children\n",
    "                if token_word.lower() == character_name.lower():\n",
    "                    associated_words.append(token_head)\n",
    "                    associated_words.extend(token_children)\n",
    "\n",
    "                # if token_word is an adjective, we want to keep it if the head is the character name\n",
    "                if token.pos_ == 'ADJ' and token_head.lower() == character_name.lower():\n",
    "                    associated_words.append(token_word)\n",
    "\n",
    "    # We want to remove from the list : character name, stopwords, punctuation, and duplicates\n",
    "    associated_words = list(set(associated_words))\n",
    "    associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "    associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "    associated_words = [word for word in associated_words if word.isalpha()]\n",
    "    \n",
    "    return associated_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58395e0f",
   "metadata": {},
   "source": [
    "# WATCHAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d63d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"Paul Watson\"\n",
    "plot = \"Paul is brilliant. He is a journalist. He is amazing. This journalist loves pizzas. Alfred is a friend of Paul. He is a good friend.\"\n",
    "\n",
    "\n",
    "associated_words = []\n",
    "characters_name_parts = character_name.lower().split()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the spacy model in english\n",
    "nlp_coref = spacy.load(\"en_coreference_web_trf\") # load the spacy model with coreference resolution\n",
    "nlp_coref.replace_listeners(\"transformer\", \"coref\", [\"model.tok2vec\"])\n",
    "nlp_coref.replace_listeners(\"transformer\", \"span_resolver\", [\"model.tok2vec\"])\n",
    "nlp.add_pipe(\"coref\", source=nlp_coref)\n",
    "nlp.add_pipe(\"span_resolver\", source=nlp_coref)\n",
    "\n",
    "doc = nlp(plot) # Tokenization\n",
    "coref_dict = doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9f2c7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clusters = int(len(coref_dict)/2)\n",
    "corref_clusters = []\n",
    "\n",
    "for i in range(nb_clusters):\n",
    "    corref_list = coref_dict[f'coref_head_clusters_{i+1}']\n",
    "    corref_list = [token.text for token in corref_list]\n",
    "    corref_clusters.append(corref_list)\n",
    "\n",
    "for corref_words in corref_clusters:\n",
    "    corref_words = [word.lower() for word in corref_words]\n",
    "    if any(name_part in corref_words for name_part in characters_name_parts):\n",
    "        corref_list = corref_words\n",
    "        break\n",
    "\n",
    "corref_list = [word for word in corref_list if word not in nlp.Defaults.stop_words]\n",
    "characters_name_parts.extend(corref_list)\n",
    "characters_name_parts = list(set(characters_name_parts))\n",
    "\n",
    "sentences = [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e39a9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watson', 'paul', 'journalist']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a349b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE NUMBER 1\n",
      "TOKEN WORD paul\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n",
      "paul is refering the character !\n",
      "We aWWWWdd to the associated words the following words: is []\n",
      "TOKEN WORD is\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN ['paul', 'brilliant', '.']\n",
      "paul is refering the character !\n",
      "TOKEN WORD brilliant\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD .\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n",
      "SENTENCE NUMBER 1\n",
      "TOKEN WORD he\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD is\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN ['he', 'journalist', '.']\n",
      "journalist is refering the character !\n",
      "TOKEN WORD a\n",
      "TOKEN HEAD journalist\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD journalist\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN ['a']\n",
      "journalist is refering the character !\n",
      "We aWWWWdd to the associated words the following words: is ['a']\n",
      "TOKEN WORD .\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n",
      "SENTENCE NUMBER 1\n",
      "TOKEN WORD this\n",
      "TOKEN HEAD journalist\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD journalist\n",
      "TOKEN HEAD loves\n",
      "TOKEN CHILDREN ['this']\n",
      "journalist is refering the character !\n",
      "We aWWWWdd to the associated words the following words: loves ['this']\n",
      "TOKEN WORD loves\n",
      "TOKEN HEAD loves\n",
      "TOKEN CHILDREN ['journalist', 'pizzas', '.']\n",
      "journalist is refering the character !\n",
      "TOKEN WORD pizzas\n",
      "TOKEN HEAD loves\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD .\n",
      "TOKEN HEAD loves\n",
      "TOKEN CHILDREN []\n",
      "SENTENCE NUMBER 1\n",
      "TOKEN WORD alfred\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD is\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN ['alfred', 'friend', '.']\n",
      "TOKEN WORD a\n",
      "TOKEN HEAD friend\n",
      "TOKEN CHILDREN []\n",
      "TOKEN WORD friend\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN ['a', 'of']\n",
      "TOKEN WORD of\n",
      "TOKEN HEAD friend\n",
      "TOKEN CHILDREN ['paul']\n",
      "paul is refering the character !\n",
      "TOKEN WORD paul\n",
      "TOKEN HEAD of\n",
      "TOKEN CHILDREN []\n",
      "paul is refering the character !\n",
      "We aWWWWdd to the associated words the following words: of []\n",
      "TOKEN WORD .\n",
      "TOKEN HEAD is\n",
      "TOKEN CHILDREN []\n"
     ]
    }
   ],
   "source": [
    "for sentence in sentences:\n",
    "\n",
    "    sentence_text = sentence.lower()\n",
    "\n",
    "    if any(name_part in sentence_text for name_part in characters_name_parts): # we want to focus on the sentences where the character appears\n",
    "\n",
    "        nb = 1\n",
    "        print(\"SENTENCE NUMBER\", nb)\n",
    "    \n",
    "        for token in nlp(sentence_text):\n",
    "\n",
    "            token_word = token.text\n",
    "            token_head = token.head.text\n",
    "            token_children = [child.text for child in token.children]\n",
    "\n",
    "            print(\"TOKEN WORD\", token_word)\n",
    "            print(\"TOKEN HEAD\", token_head)\n",
    "            print(\"TOKEN CHILDREN\", token_children) \n",
    "                \n",
    "\n",
    "            # if token_word is refering the character, we want to keep the head and the children\n",
    "            if token_word.lower() in characters_name_parts:\n",
    "                associated_words.append(token_head)\n",
    "                associated_words.extend(token_children)\n",
    "\n",
    "                print(token_word, \"is refering the character !\")\n",
    "                print(\"We aWWWWdd to the associated words the following words:\", token_head, token_children)\n",
    "\n",
    "            # if token_word is an adjective, we want to keep it if the head is refering the character\n",
    "            if token.pos_ == 'ADJ' and token_head.lower() in characters_name_parts:\n",
    "                associated_words.append(token_word)\n",
    "                print(token_word, \"is an adjective refering the character !\")\n",
    "                print(\"We add to the associated words the following word:\", token_word)\n",
    "\n",
    "            # if one of the children is reffering the character, we want to keep the other children\n",
    "            for child in token_children:\n",
    "                if child.lower() in characters_name_parts:\n",
    "                    associated_words.extend(token_children)\n",
    "                    print(child, \"is refering the character !\")\n",
    "        nb += 1\n",
    "\n",
    "\n",
    "associated_words = [word.lower() for word in associated_words]\n",
    "associated_words.extend(characters_name_parts)\n",
    "# We want to remove from the list : character name, stopwords, punctuation, and duplicates\n",
    "associated_words = list(set(associated_words))\n",
    "associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "associated_words = [word for word in associated_words if word.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a8a098cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watson', 'paul', 'journalist']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "aebd839c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watson', 'paul', 'journalist', 'brilliant', 'loves', 'pizzas']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "associated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "53ee4db6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watson', 'paul', 'journalist']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_name_parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb5aa9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paul is brilliant.',\n",
       " 'He is a journalist.',\n",
       " 'He is amazing.',\n",
       " 'This journalist loves pizzas.',\n",
       " 'Alfred is a friend of Paul.',\n",
       " 'He is a good friend.']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ceb8816e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\llitz\\anaconda3\\envs\\ada\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "# for sentence in sentences:\n",
    "\n",
    "#     sentence_doc = nlp(sentence)\n",
    "#     sentence_text = sentence.lower()\n",
    "\n",
    "#     if character_name.lower() in sentence: # we want to focus on the sentences where the character appears\n",
    "#         for token in nlp(sentence):\n",
    "#             token_word = token.text\n",
    "#             token_head = token.head.text\n",
    "#             token_children = [child.text for child in token.children]\n",
    "\n",
    "#             # if token_word is the character name, we want to keep the head and the children\n",
    "#             if token_word.lower() == character_name.lower():\n",
    "#                 associated_words.append(token_head)\n",
    "#                 associated_words.extend(token_children)\n",
    "\n",
    "#             # if token_word is an adjective, we want to keep it if the head is the character name\n",
    "#             if token.pos_ == 'ADJ' and token_head.lower() == character_name.lower():\n",
    "#                 associated_words.append(token_word)\n",
    "\n",
    "# # We want to remove from the list : character name, stopwords, punctuation, and duplicates\n",
    "# associated_words = list(set(associated_words))\n",
    "# associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "# associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "# associated_words = [word for word in associated_words if word.isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8b81c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alfred', 'He']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corref_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "40b387e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['paul', 'he', 'he', 'paul'], ['journalist', 'journalist'], ['alfred', 'he']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters_name_parts\n",
    "\n",
    "corref_list\n",
    "\n",
    "corref_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14508425",
   "metadata": {},
   "source": [
    "# BLAAAAAAAAAAAAAAAAAAAAAA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b21d8f",
   "metadata": {},
   "source": [
    "# DRAFT NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b30b600",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"Evelyn Sharp\"\n",
    "characters_name_parts = character_name.lower().split()\n",
    "\n",
    "print(characters_name_parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171b99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"Evelyn Sharp\"\n",
    "\n",
    "plot = \"\"\"The cunning detective, Evelyn Sharp, is tasked with solving a mysterious series of murders \n",
    "            in a small town. The local police are baffled, and there is tension in the air as each new \n",
    "            murder brings the town closer to chaos. The detective's sharp instincts lead her to suspect \n",
    "            the charming but secretive businessman, Victor Blackwell, who seems to be hiding something. \n",
    "            Meanwhile, Victor's associate, Lena Monroe, insists that he's innocent, but her odd behavior \n",
    "            raises suspicion. As Evelyn digs deeper, she uncovers a web of lies and deceit. Lena, however, \n",
    "            is not what she seems and may have a hidden agenda. Evelyn must navigate these lies carefully \n",
    "            while dealing with the town's growing fear. In the end, she discovers that the real mastermind \n",
    "            is Lena, whose jealousy and betrayal set the stage for the murders. As the truth comes to light, \n",
    "            Evelyn faces a moral dilemma — should she reveal the whole story, or protect some of the innocent \n",
    "            who were unwittingly involved?\"\"\"\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\") # load the spacy model in english\n",
    "plot = \" \".join(plot.split()) # make sure there is only one space between words\n",
    "plot = plot.lower() # lowercase\n",
    "doc = nlp(plot) # Tokenization\n",
    "\n",
    "associated_words = []\n",
    "\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "for sentence in sentences:\n",
    "    if character_name.lower() in sentence: # we want to focus on the sentences where the character appears\n",
    "        for token in nlp(sentence):\n",
    "            token_word = token.text\n",
    "            token_head = token.head.text\n",
    "            token_children = [child.text for child in token.children]\n",
    "\n",
    "\n",
    "            # if token_word is the character name, we want to keep the head and the children\n",
    "            if token_word.lower() == character_name.lower():\n",
    "                associated_words.append(token_head)\n",
    "                associated_words.extend(token_children)\n",
    "\n",
    "            # if token_word is an adjective, we want to keep it if the head is the character name\n",
    "            if token.pos_ == 'ADJ' and token_head.lower() == character_name.lower():\n",
    "                associated_words.append(token_word)\n",
    "\n",
    "            # if the character name is in a list of children, we want to keep the head and the other children\n",
    "            if character_name.lower() in token_children:\n",
    "                associated_words.append(token_head)\n",
    "                associated_words.extend([child for child in token_children if child.lower() != character_name.lower()])\n",
    "\n",
    "# We want to remove from the list : character name, stopwords, punctuation, and duplicates\n",
    "associated_words = list(set(associated_words))\n",
    "associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "associated_words = [word for word in associated_words if word.isalpha()]\n",
    "\n",
    "print(associated_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4511048",
   "metadata": {},
   "source": [
    "# DRAFT NLP GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d68327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"Paul\"\n",
    "\n",
    "plot = \"Paul is brilliant. He is a journalist. He is amazing. This journalist loves pizzas. Alfred is a friend of Paul. He is a good friend.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the spacy model in english\n",
    "nlp_corref = spacy.load(\"en_coreference_web_trf\") # load the spacy model with coreference resolution\n",
    "coref_clusters = nlp_corref(plot).spans\n",
    "\n",
    "plot = \" \".join(plot.split()) # make sure there is only one space between words\n",
    "plot = plot.lower() # lowercase\n",
    "doc = nlp(plot) # Tokenization\n",
    "\n",
    "associated_words = []\n",
    "\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "for sentence in sentences:\n",
    "    if character_name.lower() in sentence: # we want to focus on the sentences where the character appears\n",
    "        for token in nlp(sentence):\n",
    "            token_word = token.text\n",
    "            token_head = token.head.text\n",
    "            token_children = [child.text for child in token.children]\n",
    "\n",
    "\n",
    "            # if token_word is the character name, we want to keep the head and the children\n",
    "            if token_word.lower() == character_name.lower():\n",
    "                associated_words.append(token_head)\n",
    "                associated_words.extend(token_children)\n",
    "\n",
    "            # if token_word is an adjective, we want to keep it if the head is the character name\n",
    "            if token.pos_ == 'ADJ' and token_head.lower() == character_name.lower():\n",
    "                associated_words.append(token_word)\n",
    "\n",
    "            # if the character name is in a list of children, we want to keep the head and the other children\n",
    "            if character_name.lower() in token_children:\n",
    "                associated_words.append(token_head)\n",
    "                associated_words.extend([child for child in token_children if child.lower() != character_name.lower()])\n",
    "\n",
    "# We want to remove from the list : character name, stopwords, punctuation, and duplicates\n",
    "associated_words = list(set(associated_words))\n",
    "associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "associated_words = [word for word in associated_words if word.isalpha()]\n",
    "\n",
    "print(associated_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10245357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "plot = \"The glorious cats were startled by the ugly dog as it growled at them.\"\n",
    "character_name = \"cats\"\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_coreference_web_trf\")  # load the spacy model in English\n",
    "plot = \" \".join(plot.split())  # make sure there is only one space between words\n",
    "plot = plot.lower()  # lowercase the text\n",
    "doc = nlp(plot)  # Tokenization\n",
    "\n",
    "associated_words = []\n",
    "\n",
    "# Get sentences\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Process coreference (to handle pronouns and other referring terms)\n",
    "coref_clusters = doc._.coref_clusters if doc._.has_annotation(\"coref_clusters\") else []\n",
    "\n",
    "# Generate a list of referring words (pronouns or common referring words)\n",
    "referring_words = set()\n",
    "for cluster in coref_clusters:\n",
    "    for mention in cluster:\n",
    "        referring_words.add(mention.text.lower())  # add referring words like \"he\", \"she\", etc.\n",
    "\n",
    "referring_words.add(character_name.lower())  # Add the character's full name\n",
    "\n",
    "# Iterate over sentences\n",
    "for sentence in sentences:\n",
    "    # Check if the sentence contains part of the character's name or a referring word\n",
    "    if any(word in sentence.lower() for word in referring_words):\n",
    "        for token in nlp(sentence):\n",
    "            token_word = token.text\n",
    "            token_head = token.head.text\n",
    "            token_children = [child.text for child in token.children]\n",
    "\n",
    "            # If token_word is the character name, we want to keep the head and the children\n",
    "            if token_word.lower() == character_name.lower():\n",
    "                associated_words.append(token_head)\n",
    "                associated_words.extend(token_children)\n",
    "\n",
    "            # If token_word is an adjective, we want to keep it if the head is the character name\n",
    "            if token.pos_ == 'ADJ' and token_head.lower() == character_name.lower():\n",
    "                associated_words.append(token_word)\n",
    "\n",
    "# Remove unwanted words: character name, stopwords, punctuation, and duplicates\n",
    "associated_words = list(set(associated_words))\n",
    "associated_words = [word for word in associated_words if word.lower() != character_name.lower()]\n",
    "associated_words = [word for word in associated_words if word not in nlp.Defaults.stop_words]\n",
    "associated_words = [word for word in associated_words if word.isalpha()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebe470d",
   "metadata": {},
   "source": [
    "# DRAFT NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2147f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import spacy_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862f35f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp_coref = spacy.load(\"en_coreference_web_trf\")\n",
    "\n",
    "nlp_coref.replace_listeners(\"transformer\", \"coref\", [\"model.tok2vec\"])\n",
    "nlp_coref.replace_listeners(\"transformer\", \"span_resolver\", [\"model.tok2vec\"])\n",
    "\n",
    "nlp.add_pipe(\"coref\", source=nlp_coref)\n",
    "nlp.add_pipe(\"span_resolver\", source=nlp_coref)\n",
    "\n",
    "doc = nlp(\"Paul is brilliant. He is a journalist. He is amazing. This journalist loves pizzas. Alfred is a friend of Paul. He is a good friend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "089ef3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = doc.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7caa96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967d415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract first value from the dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d5c16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_name = \"Paul\"\n",
    "\n",
    "nb_clusters = int(len(dict)/2)\n",
    "corref_clusters = []\n",
    "\n",
    "for i in range(nb_clusters):\n",
    "    corref_list = dict[f'coref_head_clusters_{i+1}']\n",
    "    corref_list = [token.text for token in corref_list]\n",
    "    corref_clusters.append(corref_list)\n",
    "\n",
    "for corref_words in corref_clusters:\n",
    "    if character_name in corref_words:\n",
    "        corref_list = corref_words\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c9970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words_1 = ['Paul', 'brilliant', 'journalist', 'amazing', 'pizzas', 'Alfred', 'friend', 'good']\n",
    "list_of_words_2 = ['Cat', 'unknown', 'pursuit']\n",
    "\n",
    "dict_of_lists = {'list_1': list_of_words_1, 'list_2': list_of_words_2}\n",
    "\n",
    "nb_values = len(dict_of_lists)\n",
    "\n",
    "empty_list = []\n",
    "\n",
    "for i in range(nb_values): # add the words of each list to the empty list\n",
    "    empty_list.extend(dict_of_lists[f'list_{i+1}'])\n",
    "\n",
    "print(empty_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46577cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fa10a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68f0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(\"Paul is a brilliant journalist. He is amazing. This journalist loves pizzas\")\n",
    "doc2.spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba431860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coreferee\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load the spacy model in english\n",
    "nlp.add_pipe('coreferee')\n",
    "plot = \" \".join(plot.split()) # make sure there is only one space between words\n",
    "plot = plot.lower() # lowercase\n",
    "doc = nlp(plot) # Tokenization\n",
    "\n",
    "associated_words = []\n",
    "characters_name_parts = character_name.lower().split()\n",
    "\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "for sentence in sentences:\n",
    "\n",
    "    sentence_doc = nlp(sentence)\n",
    "    sentence_text = sentence.lower()\n",
    "\n",
    "    keep_sentence = (\n",
    "        any(name_part in sentence_text for name_part in characters_name_parts) or\n",
    "        any(token.pos_ == \"PRON\" and token._.in_coref for token in sentence_doc)\n",
    "    )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db33aa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e20a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = villain_analysis['Plot'].iloc[0]\n",
    "print(\"length of the plot: \", len(example_text))\n",
    "preprocessed_text = preprocess_text(example_text)\n",
    "print(\"length of the preprocessed plot: \", len(preprocessed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cf4cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca20f01",
   "metadata": {},
   "source": [
    "# DRAFT FURTHER CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64007307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the column 'Movie_genres' from the movies dataframe to the names_and_plot dataframe\n",
    "\n",
    "movies = movies.rename(columns={'Wikipedia_movie_ID': 'Wikipedia_Movie_ID'})\n",
    "names_plot_genres = pd.merge(names_and_plot, movies[['Wikipedia_Movie_ID', 'Movie_genres']], on='Wikipedia_Movie_ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2d3352",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_plot_genres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f1fdb1",
   "metadata": {},
   "source": [
    "# DRAFT 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
